{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "environment = gym.make(\"LunarLander-v2\")\n",
    "environment.observation_space.shape[0]\n",
    "Batch = namedtuple('Batch', ('state','next_state','action','reward','done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Avg Reward: -299.803796892\n",
      "Episode: 100, Avg Reward: -203.259505851\n",
      "Episode: 200, Avg Reward: -160.138726297\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, action_space, state_space):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_space, 128)\n",
    "        self.layer_2 = nn.Linear(128, 128)\n",
    "        self.output_layer = nn.Linear(128, action_space)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        layer1 = F.relu(self.layer_1(x))\n",
    "        layer2 = F.relu(self.layer_2(layer1))\n",
    "        return self.output_layer(layer2)\n",
    "#This agent utilizes two neural networks. One neural network acts as the DQNfor selecting the next action, \n",
    "#which utilizes Q learning. The other neural network acts as the target network, the \"true\" Q-value that the \n",
    "#DQN tries to attain/predict in order to evaluate the quality of taking that next action. Using two neural networks\n",
    "#helps prevent the problem with moving Q-targets as having a single neural network as the target and prediction\n",
    "#means that the same weights are being used, which means that as we become closer to the target value, the target \n",
    "#value shifts due to backpropagation and gradient descent.\n",
    "class Agent:\n",
    "    def __init__(self, environment, epsilon, gamma, alpha):\n",
    "        self.action_space = environment.action_space.n\n",
    "        self.state_space = environment.observation_space.shape[0]\n",
    "        self.epsilon = epsilon #probability for exploitation or exploration\n",
    "        self.gamma = gamma #discount factor\n",
    "        self.alpha = alpha #learning rates\n",
    "        self.memory = deque(maxlen=int(1e5)) #experience replay to produce better results\n",
    "        #self.targetNetwork = NeuralNetwork(self.action_space, self.state_space)\n",
    "        self.Qnetwork = NeuralNetwork(self.action_space, self.state_space)\n",
    "        self.targetNetwork = NeuralNetwork(self.action_space, self.state_space)\n",
    "        self.optimizer = optim.Adam(self.Qnetwork.parameters(), lr=self.alpha)\n",
    "    \n",
    "    #This method helps with experience replay, allowing for a more stable agent.\n",
    "    def add_to_memory(self, state, next_state, action, reward, done):\n",
    "        state = torch.from_numpy(state)\n",
    "        next_state = torch.from_numpy(next_state)\n",
    "        action = torch.tensor([[action]])\n",
    "        reward = torch.tensor([[reward]],dtype=torch.float)\n",
    "        done = torch.tensor([[done]], dtype=torch.float)\n",
    "        select_memory = Batch(state, next_state,action,reward,done)\n",
    "        self.memory.append(select_memory)\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        #exploration\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0,self.action_space-1)\n",
    "        #exploitation\n",
    "        else:\n",
    "            state = torch.from_numpy(state)\n",
    "            Q_values = self.Qnetwork.forward(state)\n",
    "            action = torch.argmax(Q_values).item()\n",
    "            return action\n",
    "        \n",
    "    def train_model(self, batch_size):\n",
    "        if(len(self.memory) < batch_size):\n",
    "            return\n",
    "        small_memory = random.sample(self.memory, batch_size)\n",
    "        small_memory = Batch(*zip(*small_memory))\n",
    "        state_matrix = torch.cat(small_memory.state)\n",
    "        next_state_matrix = torch.cat(small_memory.next_state)\n",
    "        action_matrix = torch.cat(small_memory.action)\n",
    "        reward_matrix = torch.cat(small_memory.reward)\n",
    "        done_matrix = torch.cat(small_memory.done)\n",
    "        #predicted_Q = self.Qnetwork(state_matrix).gather(1, action_matrix)\n",
    "        predicted_Q = torch.gather(self.Qnetwork(state_matrix),1, action_matrix)\n",
    "        #two neural networks to prevent oscillation (not double DQN yet)\n",
    "        #next_Q_value = self.targetNetwork(next_state_matrix).detach().max(1)[0].unsqueeze(1)\n",
    "        next_Q_value = torch.unsqueeze(self.targetNetwork(next_state_matrix).detach().max(1)[0],1)\n",
    "        target_Q = reward_matrix + self.gamma*(next_Q_value)*(1-done_matrix)\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(predicted_Q, target_Q)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for target_param, local_param in zip(self.targetNetwork.parameters(), self.Qnetwork.parameters()):\n",
    "            target_param.data.copy_((1e-3)*local_param.data + (1.0-(1e-3))*target_param.data)\n",
    "\n",
    "\n",
    "def testing(environment, time_steps):\n",
    "    agent = Agent(environment,epsilon=1.0,gamma=0.99,alpha=5e-4)\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    episode = 0 \n",
    "    while np.mean(scores_window) < 275 or math.isnan(np.mean(scores_window)):\n",
    "    #for episode in range(episodeNum):\n",
    "        state = environment.reset()\n",
    "        state = np.reshape(state, (1,environment.observation_space.shape[0]))\n",
    "        total_reward = 0\n",
    "        for time_step in range(time_steps):\n",
    "            environment.render()\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "            agent.train_model(64)\n",
    "            next_state = np.reshape(next_state, (1,environment.observation_space.shape[0]))\n",
    "            #add to experience replay\n",
    "            agent.add_to_memory(state,next_state,action,reward,done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            agent.epsilon = max(0.01, 0.995*agent.epsilon)\n",
    "            if done:\n",
    "                break\n",
    "            agent.train_model(64)\n",
    "        scores.append(total_reward)\n",
    "        scores_window.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(\"Episode: {}, Avg Reward: {}\".format(episode, np.mean(scores_window)))\n",
    "        episode+=1\n",
    "    environment.close()\n",
    "    torch.save(agent.Qnetwork.state_dict(), './dqn.pth')\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    \n",
    "testing(environment,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch(environment):\n",
    "    agent = Agent(environment,epsilon=1.0,gamma=0.99,alpha=5e-4)\n",
    "    agent.Qnetwork.load_state_dict(torch.load('dqn.pth'))\n",
    "\n",
    "    for i in range(3):\n",
    "        state = environment.reset()\n",
    "        for j in range(200):\n",
    "            state = np.reshape(state, (1,environment.observation_space.shape[0]))\n",
    "            action = agent.take_action(state)\n",
    "            env.render()\n",
    "            state, reward, done, _ = environment.step(action)\n",
    "            if done:\n",
    "                break        \n",
    "    environment.close()\n",
    "watch(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
